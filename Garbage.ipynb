{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agatalango/real_garbage_collector/blob/main/Garbage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project name: *Computer vision model recognizing different types of garbage using openCV.*\n",
        "\n",
        "Team members: **Katarzyna Iwaszkiewicz, Agata Lango, Dominik ZiÄ™ba**\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/mostafaabla/garbage-classification/data\n",
        "\n",
        "GIT repo: https://github.com/agatalango/real_garbage_collector\n",
        "\n"
      ],
      "metadata": {
        "id": "X1yZArWAo-CD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import required libraries"
      ],
      "metadata": {
        "id": "3vTchNALvZs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil, random\n",
        "import cv2\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image as im\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "#from tf.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from PIL import Image as im\n",
        "import cv2"
      ],
      "metadata": {
        "id": "ZogbZXl6Be0a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a dictionary from 12 classes"
      ],
      "metadata": {
        "id": "C6f3L7M0vh5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root='https://github.com/agatalango/real_garbage_collector/tree/513457570c0234356449cb1d61fc5451994e7149/garbage_classification'\n",
        "data={}\n",
        "for i in os.listdir(root):\n",
        "    for j in os.walk(root+i):\n",
        "        for k in j[2]:\n",
        "            data[root+i+'/'+k]=i\n",
        "data=pd.DataFrame(data.items(),columns=['path','class_'])\n",
        "data['class_'].value_counts().plot(kind='bar')\n",
        "plt.title('Data 12 Class (VERY UNBALANCED)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "sUTZVw7PGDGd",
        "outputId": "71df2140-3b88-4387-d5ab-fedc3de118ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https://github.com/agatalango/real_garbage_collector/tree/513457570c0234356449cb1d61fc5451994e7149/garbage_classification'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-708a74613568>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://github.com/agatalango/real_garbage_collector/tree/513457570c0234356449cb1d61fc5451994e7149/garbage_classification'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://github.com/agatalango/real_garbage_collector/tree/513457570c0234356449cb1d61fc5451994e7149/garbage_classification'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "base_path = \"https://github.com/agatalango/real_garbage_collector/tree/main/garbage_classification\"\n",
        "\n",
        "categories = {0: 'paper', 1: 'cardboard', 2: 'plastic', 3: 'metal', 4: 'trash', 5: 'glass'}\n",
        "\n",
        "print('defining constants successful!')\n",
        "\n",
        "#Add class name prefix to filename. So for example \"/paper104.jpg\" become \"paper/paper104.jpg\"\n",
        "def add_class_name_prefix(df, col_name):\n",
        "    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n",
        "    return df\n",
        "\n",
        "base_path = \"/garbage-classification/garbage_classification/\"\n",
        "# Creating a dictionary from 12 classes\n",
        "categories={}\n",
        "i=0\n",
        "for dirname, _, filenames in os.walk('https://github.com/agatalango/real_garbage_collector/tree/513457570c0234356449cb1d61fc5451994e7149/garbage_classification'):\n",
        "    for filename in filenames:\n",
        "        categories[i] = dirname.split('/')[-1]\n",
        "        break\n",
        "        print(os.path.join(dirname, filename))\n",
        "    i += 1\n",
        "print(categories)\n",
        "print('defining constants successful!')"
      ],
      "metadata": {
        "id": "WrXhO1f6gySr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4eb843-74d9-4094-853b-c2d7e8f1ad21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defining constants successful!\n",
            "{}\n",
            "defining constants successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add class name prefix to filename. So for example \"/paper104.jpg\" become \"paper/paper104.jpg\"\n",
        "#def add_class_name_prefix(df, col_name):\n",
        "#    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n",
        "#    return df"
      ],
      "metadata": {
        "id": "-arbOFVGi4UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FEtF3mrO_F6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Create dataframe"
      ],
      "metadata": {
        "id": "PSr_U6uZvw3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linkcode\n",
        "import os, shutil, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "BZe3iBYh_cgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(\"/kaggle/working/\", \"train\")\n",
        "test_dir = os.path.join(\"/kaggle/working/\", \"test\")\n",
        "origin_dir = os.path.join(\"/kaggle\", \"input\", \"garbage-classification\",  \"garbage_classification\")\n",
        "categories = os.listdir(origin_dir)\n",
        "\n",
        "for category in categories:\n",
        "    category_src_path = os.path.join(origin_dir, category)\n",
        "    category_dst_path_train = os.path.join(train_dir, category)\n",
        "    category_dst_path_test = os.path.join(test_dir, category)\n",
        "    os.makedirs(category_dst_path_train, exist_ok = True)\n",
        "    os.makedirs(category_dst_path_test, exist_ok = True)\n",
        "\n",
        "    file_names = os.listdir(category_src_path)\n",
        "    random.shuffle(file_names)\n",
        "    length = len(file_names)\n",
        "    train_file_names = file_names[:int(length*0.8)]\n",
        "    test_file_names = file_names[int(length*0.8):]\n",
        "\n",
        "    for name in train_file_names:\n",
        "        src_path = os.path.join(category_src_path, name)\n",
        "        dst_path = os.path.join(category_dst_path_train, name)\n",
        "        shutil.copyfile(src = src_path,\n",
        "                        dst = dst_path)\n",
        "\n",
        "    for name in test_file_names:\n",
        "        src_path = os.path.join(category_src_path, name)\n",
        "        dst_path = os.path.join(category_dst_path_test, name)\n",
        "        shutil.copyfile(src = src_path,\n",
        "                        dst = dst_path)"
      ],
      "metadata": {
        "id": "gfbwg8WQ_PDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                                                             validation_split = 0.2)\n",
        "test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_data = train_gen.flow_from_directory(\"/kaggle/working/train\",\n",
        "                                            subset = \"training\", seed = 42, target_size = (299, 299))\n",
        "val_data = train_gen.flow_from_directory(\"/kaggle/working/train\",\n",
        "                                            subset = \"validation\", seed = 42, target_size = (299, 299))\n",
        "test_data = test_gen.flow_from_directory(\"/kaggle/working/test\", seed = 42, target_size = (299, 299))"
      ],
      "metadata": {
        "id": "znY0Njau_QJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = {}\n",
        "\n",
        "train_path = os.path.join(\"/kaggle/working/train\")\n",
        "for cls in os.listdir(train_path):\n",
        "    class_path = os.path.join(train_path, cls)\n",
        "    class_counts[cls] = len(os.listdir(class_path))\n",
        "\n",
        "class_counts = dict(sorted(class_counts.items(), key=lambda x:x[1], reverse = True))\n",
        "class_counts"
      ],
      "metadata": {
        "id": "0-2BQn9M_i2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.barplot(x = list(class_counts.values()), y = list(class_counts.keys()))"
      ],
      "metadata": {
        "id": "ex13-DOE_mDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(train_data.class_indices.keys())\n",
        "class_names"
      ],
      "metadata": {
        "id": "Rf92tRzn_xxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### list conatining all the filenames in the dataset\n",
        "filenames_list = []\n",
        "# list to store the corresponding category, note that each folder of the dataset has one class of data\n",
        "categories_list = []\n",
        "\n",
        "for category in categories:\n",
        "    filenames = os.listdir(base_path + categories[category])\n",
        "\n",
        "    filenames_list = filenames_list  +filenames\n",
        "    categories_list = categories_list + [category] * len(filenames)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'filename': filenames_list,\n",
        "    'category': categories_list\n",
        "})\n",
        "\n",
        "df = add_class_name_prefix(df, 'filename')\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print('number of elements = ' , len(df))\n",
        "\n"
      ],
      "metadata": {
        "id": "3ftD9u9ykJ3Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "627ed4bd-5c3e-4189-cc20-effbac688a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'categories' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c82629969f4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcategories_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Bz3nIGIBuv8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see sample image, you can run the same cell again to get a different image\n",
        "random_row = random.randint(0, len(df)-1)\n",
        "sample = df.iloc[random_row]\n",
        "randomimage = image.load_img(base_path +sample['filename'])"
      ],
      "metadata": {
        "id": "_YgHvUwMu-Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualize the categories distribution\n"
      ],
      "metadata": {
        "id": "zUav3MNcv27u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_visualization = df.copy()\n",
        "# Change the catgegories from numbers to names\n",
        "df_visualization['category'] = df_visualization['category'].apply(lambda x:categories[x] )\n",
        "\n",
        "df_visualization['category'].value_counts().plot.bar(x = 'count', y = 'category' )\n",
        "\n",
        "plt.xlabel(\"Garbage Classes\", labelpad=14)\n",
        "plt.ylabel(\"Images Count\", labelpad=14)\n",
        "plt.title(\"Count of images per class\", y=1.02);"
      ],
      "metadata": {
        "id": "d6Jo1uqOkJQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Split dataset into train, test and validation sets: 80% train set, 10% cross_validation set, and 10% test set (TBC)"
      ],
      "metadata": {
        "id": "SZXsfDEnwFKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the categories from numbers to names\n",
        "df[\"category\"] = df[\"category\"].replace(categories)\n",
        "\n",
        "# We first split the data into two sets and then split the validate_df to two sets\n",
        "train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "total_train = train_df.shape[0]\n",
        "total_validate = validate_df.shape[0]\n",
        "\n",
        "print('train size = ', total_validate , 'validate size = ', total_validate, 'test size = ', test_df.shape[0])"
      ],
      "metadata": {
        "id": "tjayD_QEivF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Create model"
      ],
      "metadata": {
        "id": "WNtvpFhcwB0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "import keras.applications.xception as xception\n",
        "\n",
        "xception_layer = xception.Xception(include_top = False, input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS),\n",
        "                       weights = '../input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "\n",
        "# We don't want to train the imported weights\n",
        "xception_layer.trainable = False\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
        "\n",
        "#create a custom layer to apply the preprocessing\n",
        "def xception_preprocessing(img):\n",
        "  return xception.preprocess_input(img)\n",
        "\n",
        "model.add(Lambda(xception_preprocessing))\n",
        "\n",
        "model.add(xception_layer)\n",
        "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "model.add(Dense(len(categories), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TNJ59NLtkdcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "...\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//batch_size,\n",
        "    steps_per_epoch=total_train//batch_size,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "-GeGkQJllTDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "DCgmRYeTwKdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "\n",
        "    ###  Augmentation Start  ###\n",
        "\n",
        "    rotation_range=30,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = True,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2\n",
        "\n",
        "    ##  Augmentation End  ###\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    base_path,\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "wVpsFyKrwN5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_datagen = image.ImageDataGenerator()\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    validate_df,\n",
        "    base_path,\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "jG4wA98awU1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 7\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//batch_size,\n",
        "    steps_per_epoch=total_train//batch_size,\n",
        "    #callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "YL4UEPaCwV7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"model12.h5\")"
      ],
      "metadata": {
        "id": "-uDgJ9K4wZxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Visualize training process"
      ],
      "metadata": {
        "id": "f1OS-RaYwJfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
        "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
        "ax1.set_yticks(np.arange(0, 0.7, 0.1))\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(history.history['categorical_accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax2.plot(history.history['val_categorical_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "ax2.legend()\n",
        "\n",
        "legend = plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7q6Z9nskMpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Validation of training process"
      ],
      "metadata": {
        "id": "sPnu_qvWsAQN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agEfQdk_kb4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Save model hdf5"
      ],
      "metadata": {
        "id": "0nrqopvIolwD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6O2nUHK7kcaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}